{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数组a的值为：\n",
      "[[-0.69314718 -1.60943791 -1.2039728 ]\n",
      " [-0.22314355 -2.30258509 -2.30258509]]\n",
      "找出a中每行最大值：\n",
      "[[-0.69314718]\n",
      " [-0.22314355]]\n",
      "a中每行均减去本行最大值后的数组b：\n",
      "[[ 0.         -0.91629073 -0.51082562]\n",
      " [ 0.         -2.07944154 -2.07944154]]\n",
      "对数组a进行softmax：\n",
      "[[ 0.5  0.2  0.3]\n",
      " [ 0.8  0.1  0.1]]\n",
      "对去掉最大值的进行softmax：\n",
      "[[ 0.5  0.2  0.3]\n",
      " [ 0.8  0.1  0.1]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "a = np.array([[-0.69314718 ,-1.60943791, -1.2039728],[-0.22314355, -2.30258509, -2.30258509]])\n",
    "print (\"数组a的值为：\\n\" + str(a))\n",
    "print (\"找出a中每行最大值：\")\n",
    "print (np.max(a, axis=1).reshape(-1,1))\n",
    "b = a - np.max(a, axis=1).reshape(-1, 1)\n",
    "print (\"a中每行均减去本行最大值后的数组b：\")\n",
    "print (b)\n",
    "a_softmax = np.exp(a) / np.sum(np.exp(a), axis=1).reshape(-1, 1)\n",
    "print (\"对数组a进行softmax：\")\n",
    "print (a_softmax)\n",
    "b_softmax = np.exp(b) / np.sum(np.exp(b), axis=1).reshape(-1, 1)\n",
    "print (\"对去掉最大值的进行softmax：\")\n",
    "print (b_softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到减不减去每行的最大值对softmax结果都没有影响，但是这里softmax之前还是要加这一步骤，为了去除数据里的噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们上次得到的softmax为：\n",
      "[[ 0.5  0.2  0.3]\n",
      " [ 0.8  0.1  0.1]]\n",
      "对softmax取ln：\n",
      "[[-0.30103    -0.69897    -0.52287874]\n",
      " [-0.09691001 -1.         -1.        ]]\n",
      "找出softmax中每行我们标签概率最大的两个数，也就是第一行的第0个，第二行的第0个：\n",
      "[ 0.5  0.8]\n",
      "分别对这两个数进行ln：\n",
      "[-0.30103    -0.09691001]\n",
      "最后，因为这两行是一个batch的两个，所以，加和去平均，得到的就是Loss：\n",
      "0.198970004736\n"
     ]
    }
   ],
   "source": [
    "print (\"我们上次得到的softmax为：\")\n",
    "print (b_softmax)\n",
    "d = np.log10(b_softmax)                         # log下什么都不写默认是自然对数e为底 ，np.log10()是以10为底\n",
    "print (\"对softmax取ln：\")\n",
    "print (d)\n",
    "print (\"找出softmax中每行我们标签概率最大的两个数，也就是第一行的第0个，第二行的第0个：\")\n",
    "print (b_softmax[range(2), list([0,0])])\n",
    "c = np.log10(b_softmax[range(2), list([0,0])])\n",
    "print (\"分别对这两个数进行ln：\")\n",
    "print (c)\n",
    "print (\"最后，因为这两行是一个batch的两个，所以，加和去平均，得到的就是Loss：\")\n",
    "print (-np.sum(np.log10(b_softmax[range(2), list([0,0])]))*(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
