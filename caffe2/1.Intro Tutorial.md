
# Caffe2 Concepts


# Blobs and Workspace, Tensors


下面，我们通过搭建一个简单的网络，来简单了解caffe2的Blobs and Workspace, Tensors怎么用，以及之间的关系：
我们这个网络有三层：

1.一个全连接层 (FC)

2.一个s型激活层，带有一个Softmax

3.一个交叉熵loss层



# 1.定义数据，标签；并讲数据标签放入workspace中的“data”“label”中


```python
from caffe2.python import workspace, model_helper
import numpy as np

# Create the input data
data = np.random.rand(16, 100).astype(np.float32)

# Create labels for the data as integers [0, 9].
label = (np.random.rand(16) * 10).astype(np.int32)

workspace.FeedBlob("data", data)
workspace.FeedBlob("label", label)

```




    True



# 2.使用model_helper新建一个model


```python
# Create model using a model helper
m = model_helper.ModelHelper(name="my_first_net")
```

ModelHelper将会创建两个相关的net：

1.初始化相关参数的网络(ref. init_net)

2.执行实际训练的网络(ref. exec_net)


# 3.新建FC层

## 3.1 新建operators之前，先定义w，b


```python
weight = m.param_init_net.XavierFill([], 'fc_w', shape=[10, 100])
bias = m.param_init_net.ConstantFill([], 'fc_b', shape=[10, ])
```

## 3.2 给m这个net新建一个FC的operators


```python
fc_1 = m.net.FC(["data", "fc_w", "fc_b"], "fc1")
```

# 4.新建激活层和softmax层


```python
pred = m.net.Sigmoid(fc_1, "pred")
softmax, loss = m.net.SoftmaxWithLoss([pred, "label"], ["softmax", "loss"])
```

注意：

1.我们这里batch_size = 16，也就是一次16个samples同时训练；

2.我们这里只是创建模型的定义，下一步开始训练；

3.model会存储在一个protobuf structure中，我们可以通过以下命令查看model：


```python
print(m.net.Proto())
```

    name: "my_first_net"
    op {
      input: "data"
      input: "fc_w"
      input: "fc_b"
      output: "fc1"
      name: ""
      type: "FC"
    }
    op {
      input: "fc1"
      output: "pred"
      name: ""
      type: "Sigmoid"
    }
    op {
      input: "pred"
      input: "label"
      output: "softmax"
      output: "loss"
      name: ""
      type: "SoftmaxWithLoss"
    }
    external_input: "data"
    external_input: "fc_w"
    external_input: "fc_b"
    external_input: "label"
    


**查看初始化的参数:**


```python
print(m.param_init_net.Proto())
```

    name: "my_first_net_init"
    op {
      output: "fc_w"
      name: ""
      type: "XavierFill"
      arg {
        name: "shape"
        ints: 10
        ints: 100
      }
    }
    op {
      output: "fc_b"
      name: ""
      type: "ConstantFill"
      arg {
        name: "shape"
        ints: 10
      }
    }
    


# 5.执行

## 5.1.运行一次参数初始化：


```python
workspace.RunNetOnce(m.param_init_net)
```




    True



## 5.2.创建训练网络


```python
workspace.CreateNet(m.net)
```




    True



## 5.3.训练网络


```python
# Run 100 x 10 iterations
for _ in range(100):
    data = np.random.rand(16, 100).astype(np.float32)
    label = (np.random.rand(16) * 10).astype(np.int32)

    workspace.FeedBlob("data", data)
    workspace.FeedBlob("label", label)

    workspace.RunNet(m.name, 10)   # run for 10 times
```

执行后，您可以检查存储在输出blob（包含张量，即numpy数组）中的结果：


```python
print(workspace.FetchBlob("softmax"))
print(workspace.FetchBlob("loss"))
```

    [[0.0876148  0.07548364 0.09623323 0.12869014 0.08200206 0.10208801
      0.12435256 0.11641801 0.09138598 0.09573159]
     [0.08656767 0.08965836 0.10828611 0.1141852  0.09495509 0.08777543
      0.11474496 0.10817766 0.10231902 0.09333058]
     [0.08370095 0.07577838 0.10283273 0.12051839 0.09019332 0.1001257
      0.12431574 0.10195971 0.10198188 0.09859312]
     [0.08037042 0.08022888 0.1113458  0.12137544 0.09024613 0.09120934
      0.11656947 0.10384342 0.10305068 0.10176046]
     [0.0878759  0.07796903 0.10136125 0.12322115 0.0864681  0.09394205
      0.11819859 0.11712413 0.08619776 0.10764208]
     [0.08172415 0.07318036 0.0948612  0.13026348 0.09646249 0.10189744
      0.10387536 0.1055788  0.08963021 0.12252647]
     [0.07917343 0.08171824 0.11740194 0.11786009 0.09542355 0.10089807
      0.10691947 0.08718182 0.09749852 0.11592486]
     [0.09023243 0.08145688 0.09926067 0.11211696 0.09406143 0.09656373
      0.10264882 0.11141657 0.09514029 0.11710224]
     [0.08539043 0.08547576 0.10477988 0.11539709 0.08580235 0.09573507
      0.11900022 0.11075822 0.09151191 0.106149  ]
     [0.08888969 0.07497442 0.1067399  0.12012165 0.0906563  0.09574833
      0.11331636 0.09551872 0.09023768 0.12379707]
     [0.07795423 0.07904412 0.10365619 0.1125275  0.08462506 0.09732373
      0.12132311 0.10707201 0.09885976 0.11761425]
     [0.08336885 0.07465281 0.10840832 0.11558709 0.08637662 0.11698949
      0.10753863 0.10991579 0.08486839 0.11229403]
     [0.0904911  0.08106896 0.10705142 0.11207567 0.1001352  0.08801412
      0.10004795 0.10290649 0.09353973 0.1246694 ]
     [0.08747555 0.07783487 0.11689501 0.11241519 0.08192006 0.09484729
      0.11507696 0.10664993 0.10004598 0.10683927]
     [0.08289554 0.08357728 0.10166691 0.11176678 0.08568645 0.10207526
      0.1217039  0.10599326 0.09480698 0.10982765]
     [0.07501717 0.0773883  0.09515747 0.12306631 0.1001178  0.09227181
      0.12529895 0.09527896 0.10404698 0.11235628]]
    2.3265471


# 6.反向传播

这个网络只包含前向传播，因此它不会学习任何东西。通过在正向传递中为每个运算符添加gradient operators来创建向后传递。


```python
from caffe2.python import workspace, model_helper
import numpy as np

# Create the input data
data = np.random.rand(16, 100).astype(np.float32)

# Create labels for the data as integers [0, 9].
label = (np.random.rand(16) * 10).astype(np.int32)

workspace.FeedBlob("data", data)
workspace.FeedBlob("label", label)

# Create model using a model helper
m = model_helper.ModelHelper(name="my_first_net")


weight = m.param_init_net.XavierFill([], 'fc_w', shape=[10, 100])
bias = m.param_init_net.ConstantFill([], 'fc_b', shape=[10, ])


fc_1 = m.net.FC(["data", "fc_w", "fc_b"], "fc1")
pred = m.net.Sigmoid(fc_1, "pred")
softmax, loss = m.net.SoftmaxWithLoss([pred, "label"], ["softmax", "loss"])

# 反向传播 add ================
m.AddGradientOperators([loss])
# ============================

workspace.RunNetOnce(m.param_init_net)
workspace.CreateNet(m.net)

# 可以查看加了反向传播之后，net中多出来三个operators的反向传播层
print(m.net.Proto())
#=========================================================

# Run 100 x 10 iterations
for _ in range(100):
    data = np.random.rand(16, 100).astype(np.float32)
    label = (np.random.rand(16) * 10).astype(np.int32)

    workspace.FeedBlob("data", data)
    workspace.FeedBlob("label", label)

    workspace.RunNet(m.name, 10)   # run for 10 times

print(workspace.FetchBlob("softmax"))
print(workspace.FetchBlob("loss"))

```

    name: "my_first_net_4"
    op {
      input: "data"
      input: "fc_w"
      input: "fc_b"
      output: "fc1"
      name: ""
      type: "FC"
    }
    op {
      input: "fc1"
      output: "pred"
      name: ""
      type: "Sigmoid"
    }
    op {
      input: "pred"
      input: "label"
      output: "softmax"
      output: "loss"
      name: ""
      type: "SoftmaxWithLoss"
    }
    op {
      input: "loss"
      output: "loss_autogen_grad"
      name: ""
      type: "ConstantFill"
      arg {
        name: "value"
        f: 1.0
      }
    }
    op {
      input: "pred"
      input: "label"
      input: "softmax"
      input: "loss_autogen_grad"
      output: "pred_grad"
      name: ""
      type: "SoftmaxWithLossGradient"
      is_gradient_op: true
    }
    op {
      input: "pred"
      input: "pred_grad"
      output: "fc1_grad"
      name: ""
      type: "SigmoidGradient"
      is_gradient_op: true
    }
    op {
      input: "data"
      input: "fc_w"
      input: "fc1_grad"
      output: "fc_w_grad"
      output: "fc_b_grad"
      output: "data_grad"
      name: ""
      type: "FCGradient"
      is_gradient_op: true
    }
    external_input: "data"
    external_input: "fc_w"
    external_input: "fc_b"
    external_input: "label"
    
    [[0.11452593 0.10101821 0.09836449 0.0965292  0.10351954 0.11437476
      0.0841739  0.08410054 0.1032382  0.10015523]
     [0.10163245 0.09582136 0.10728257 0.09061375 0.09165412 0.10958022
      0.09790237 0.08632593 0.10816205 0.11102515]
     [0.11153644 0.09187157 0.08876271 0.10034752 0.10996686 0.10122422
      0.09731211 0.07600702 0.10907148 0.11390015]
     [0.10677353 0.09250082 0.09395255 0.09013956 0.11205406 0.10125393
      0.0998921  0.08900981 0.10824097 0.10618276]
     [0.10816263 0.09636337 0.09732535 0.09815469 0.10086755 0.10827011
      0.09060381 0.07663068 0.10915532 0.11446647]
     [0.10561427 0.09925039 0.09791987 0.09596141 0.10207485 0.10818447
      0.09482063 0.08366729 0.09857398 0.11393285]
     [0.09989694 0.10426529 0.10590892 0.09590165 0.10164034 0.10572004
      0.0843733  0.07041462 0.11091355 0.12096539]
     [0.11713242 0.09610002 0.09875591 0.09168133 0.1124823  0.09706804
      0.09423647 0.07736441 0.09833553 0.11684357]
     [0.11634789 0.10220942 0.09840939 0.08635401 0.11169235 0.09972424
      0.0881576  0.07687011 0.11021466 0.11002035]
     [0.0986582  0.10288956 0.09780209 0.09394148 0.10023048 0.11133306
      0.09275422 0.07090774 0.11865855 0.11282461]
     [0.11411306 0.09094301 0.0921051  0.09122376 0.10246258 0.09556931
      0.10687889 0.0793411  0.10633816 0.12102505]
     [0.11857571 0.10367715 0.0916706  0.09522462 0.10553856 0.1097901
      0.08682371 0.08298165 0.10331775 0.10240018]
     [0.10449859 0.09788019 0.08315773 0.09780625 0.10613973 0.10121298
      0.09945863 0.07901601 0.103154   0.12767594]
     [0.0995274  0.10280239 0.0894575  0.09834379 0.1094081  0.09976058
      0.10002778 0.07689121 0.11048673 0.1132945 ]
     [0.10373392 0.09080214 0.09258449 0.09425651 0.12741047 0.10679753
      0.09512977 0.073976   0.10132045 0.11398876]
     [0.09918068 0.10451347 0.08183451 0.11167922 0.11003976 0.09900602
      0.08880362 0.08632898 0.11165588 0.10695779]]
    2.2832193



```python

```
